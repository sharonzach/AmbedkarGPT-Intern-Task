# AmbedkarGPT-Intern-Task

A simple Retrieval-Augmented Generation (RAG) prototype built for the **Kalpit Pvt Ltd â€“ AI Intern Hiring Assignment**.  
The system loads a speech by Dr. B.R. Ambedkar, creates embeddings, stores them in a local vector database, retrieves the most relevant chunks based on a user query, and generates an answer using a **local Ollama model (llama3.2:1b)**.

This solution runs **fully offline** using:
- LangChain (latest modular version)
- ChromaDB (local vector store)
- HuggingFace MiniLM embeddings
- Ollama (model: `llama3.2:1b` â€” very lightweight and works on low RAM)
- Python 3.8+ (WSL recommended on Windows)

---

## ğŸš€ Features

- Loads and processes `speech.txt`
- Splits speech into semantic chunks
- Generates embeddings using HuggingFace MiniLM
- Stores embeddings in ChromaDB
- Retrieves relevant chunks from the vector DB
- Uses **Ollama llama3.2:1b** to answer questions
- Fully local (no API keys, no cloud services)

---

## ğŸ“ Project Structure

AmbedkarGPT-Intern-Task/
â”‚â”€â”€ main.py
â”‚â”€â”€ speech.txt
â”‚â”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ§  Model Used

Why this model?  
âœ” Very lightweight (~1B parameters)  
âœ” Only ~1.2GB RAM needed  
âœ” Works smoothly inside WSL on low-memory systems  
âœ” Fast inference compared to Mistral 7B  

---

## âš™ï¸ Installation Instructions

### 1ï¸âƒ£ Install WSL (Windows Users Only)

Open **PowerShell (Admin)**:


Restart PC â†’ Open Ubuntu.

---

### 2ï¸âƒ£ Install Ollama inside WSL

curl -fsSL https://ollama.ai/install.sh
 | sh

Then pull the lightweight model:

ollama pull llama3.2:1b

Test:

ollama run llama3.2:1b

---

### 3ï¸âƒ£ Create and Activate a Python Virtual Environment

python3 -m venv venv
source venv/bin/activate


---

### 4ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

---

## â–¶ï¸ Running the Application

Inside the project folder:

python3 main.py


You will see:

AmbedkarGPT â€” Using llama3.2:1b
Ask questions about the speech.

Example:

**You:**  
What is the real remedy?

**Answer:**  
A correct context-based response from the text.

---

## ğŸ” Rebuilding the Vector DB (Optional)

If you want to delete old DB and regenerate fresh vectors:

rm -rf chroma_db
python3 main.py

---

## ğŸ“„ Provided Speech Text (speech.txt)

This project uses the provided excerpt from **Annihilation of Caste**.

---

## â— Important Notes

- This is a **prototype**, not a production system.
- All processing is local â€” there is **no cloud**, **no API key**, and **no paid tools**.
- Optimized for low-RAM laptops using a small LLM.

---

## âœ” Submission Deliverables

Include in your GitHub repo:

- `main.py`
- `speech.txt`
- `requirements.txt`
- Updated `README.md`

Make your repository public and name it:


